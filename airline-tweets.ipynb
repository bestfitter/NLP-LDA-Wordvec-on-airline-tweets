{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "Code is hidden for easier reading.\n",
       " <b><font size=\"3\">To toggle on/off code, \n",
       " click</font></b> <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "Code is hidden for easier reading.\n",
    " <b><font size=\"3\">To toggle on/off code, \n",
    " click</font></b> <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An NLP (LDA & Word2Vec) trend analysis (topic modelling) exercise on Tweets sent by customer to different airlines\n",
    "\n",
    "The data contains 146,000 tweets by customers to 6 different airlines' support tweet hashtag on various topics. The data is already categorized by various criterion e.g. 'positive', 'negative', 'neural' sentiment, or by issue type, 'luggage', 'reservation', 'late'. etc.\n",
    "\n",
    "However, I will only be using the 'text' field containing the customer's text, perform unsupervised training and identify the trends/issues/topics.\n",
    "\n",
    "#### Potential business uses:\n",
    "- A weekly report to airline's executive mgmt indicating the trends/issues in complaint data (from twitter channel)\n",
    "- A dialog system (chatbot)\n",
    "- Redirect, classify and summarize complaint/inquiry text from airline's support pages\n",
    "\n",
    "### Objective: To quickly train an LDA model on the 'text' field and identify trends/topics/sentiment.\n",
    "\n",
    "#### Data: The data file 'Tweets.csv' can be downloaded from Kaggle [here]( https://www.kaggle.com/crowdflower/twitter-airline-sentiment#Tweets.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and display a few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (14640, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                        0.0  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold      name negativereason_gold  retweet_count  \\\n",
       "0                    NaN   cairdin                 NaN              0   \n",
       "1                    NaN  jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "dirname='D:\\ML-Data\\\\airline-tweet-sentiment' # local directory name where file is downloaded\n",
    "filename='Tweets.csv'\n",
    "maindf=pd.read_csv(os.path.join(dirname,filename))\n",
    "\n",
    "print('shape:',maindf.shape)\n",
    "maindf.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for nulls, especially interested in the 'text' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id                            0\n",
       "airline_sentiment                   0\n",
       "airline_sentiment_confidence        0\n",
       "negativereason                   5462\n",
       "negativereason_confidence        4118\n",
       "airline                             0\n",
       "airline_sentiment_gold          14600\n",
       "name                                0\n",
       "negativereason_gold             14608\n",
       "retweet_count                       0\n",
       "text                                0\n",
       "tweet_coord                     13621\n",
       "tweet_created                       0\n",
       "tweet_location                   4733\n",
       "user_timezone                    4820\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maindf.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Three columns have almost 90% nulls, drop them. Also drop 'name' of person column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns dropped\n"
     ]
    }
   ],
   "source": [
    "maindf.drop(labels=['airline_sentiment_gold','negativereason_gold','tweet_coord', 'name'], axis='columns',inplace=True)\n",
    "print('Columns dropped')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check some categorical columns and their counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical cols: ['airline_sentiment', 'negativereason', 'airline', 'tweet_location']\n",
      "*********** airline_sentiment\n",
      "negative    9178\n",
      "neutral     3099\n",
      "positive    2363\n",
      "Name: airline_sentiment, dtype: int64\n",
      "*********** negativereason\n",
      "Customer Service Issue         2910\n",
      "Late Flight                    1665\n",
      "Can't Tell                     1190\n",
      "Cancelled Flight                847\n",
      "Lost Luggage                    724\n",
      "Bad Flight                      580\n",
      "Flight Booking Problems         529\n",
      "Flight Attendant Complaints     481\n",
      "longlines                       178\n",
      "Damaged Luggage                  74\n",
      "Name: negativereason, dtype: int64\n",
      "*********** airline\n",
      "United            3822\n",
      "US Airways        2913\n",
      "American          2759\n",
      "Southwest         2420\n",
      "Delta             2222\n",
      "Virgin America     504\n",
      "Name: airline, dtype: int64\n",
      "*********** tweet_location\n",
      "Boston, MA                        157\n",
      "New York, NY                      156\n",
      "Washington, DC                    150\n",
      "New York                          127\n",
      "USA                               126\n",
      "Chicago                           104\n",
      "New York City                      96\n",
      "Los Angeles, CA                    96\n",
      "NYC                                95\n",
      "San Francisco, CA                  91\n",
      "San Francisco                      86\n",
      "Chicago, IL                        81\n",
      "Brooklyn, NY                       66\n",
      "Austin, TX                         64\n",
      "Los Angeles                        64\n",
      "Washington, D.C.                   63\n",
      "Boston                             62\n",
      "Dallas, TX                         54\n",
      "Washington DC                      53\n",
      "Nashville, TN                      45\n",
      "NY                                 42\n",
      "Texas                              42\n",
      "Philadelphia, PA                   38\n",
      "San Diego                          38\n",
      "Denver, CO                         37\n",
      "Houston, TX                        35\n",
      "Global                             34\n",
      "Seattle                            34\n",
      "Logan International Airport        32\n",
      "New York, New York                 31\n",
      "                                 ... \n",
      "Somewhere East                      1\n",
      "Melbourne, Aus                      1\n",
      "Between FL & MN                     1\n",
      "Douglas, MA                         1\n",
      "Brownsville, Tx                     1\n",
      "Pacific Northwest                   1\n",
      "Fort Lauderdale, Fl                 1\n",
      "Summit, NJ                          1\n",
      "Ottawa, Ontario, The Universe       1\n",
      "Mount Dora, Florida                 1\n",
      "STX ✈️ MIA                          1\n",
      "0/5 0/4                             1\n",
      "Cork.Ireland                        1\n",
      "Phila, Princeton, NYC.              1\n",
      "Wien                                1\n",
      "State College, PA                   1\n",
      "Lower Pacific Heights, SF, CA       1\n",
      "SF ↔ NY                             1\n",
      "San Francisco & Scottsdale          1\n",
      "P.R.O.B.                            1\n",
      "Mysore : London : New York          1\n",
      "Buffalo, New York, Florida          1\n",
      "ÜT: 37.427592,-122.116357           1\n",
      "usually home                        1\n",
      "Bangor, Maine, USA                  1\n",
      "Out and About - Homebase is MD      1\n",
      "... (shrugs)                        1\n",
      "Santa Barbara CA                    1\n",
      "#avgeek, United 1K                  1\n",
      "#yeahTHATgreenville SC              1\n",
      "Name: tweet_location, Length: 3081, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "cat_cols=[col for col in maindf.select_dtypes('object').columns if col not in ['user_timezone','tweet_created','text']]\n",
    "print('categorical cols:',cat_cols)\n",
    "for col in cat_cols:\n",
    "    print('***********',col)\n",
    "    print(maindf[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check a few 'text' column entries for all the different airlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          airline                                               text\n",
       "0  Virgin America                @VirginAmerica What @dhepburn said.\n",
       "1  Virgin America  @VirginAmerica plus you've added commercials t..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>United</td>\n",
       "      <td>@united thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>United</td>\n",
       "      <td>@united Thanks for taking care of that MR!! Ha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    airline                                               text\n",
       "504  United                                     @united thanks\n",
       "505  United  @united Thanks for taking care of that MR!! Ha..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4326</th>\n",
       "      <td>Southwest</td>\n",
       "      <td>@SouthwestAir still waiting. Just hit one hour.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4327</th>\n",
       "      <td>Southwest</td>\n",
       "      <td>@SouthwestAir although I'm not happy you Cance...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        airline                                               text\n",
       "4326  Southwest    @SouthwestAir still waiting. Just hit one hour.\n",
       "4327  Southwest  @SouthwestAir although I'm not happy you Cance..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6746</th>\n",
       "      <td>Delta</td>\n",
       "      <td>@JetBlue Yesterday on my way from EWR to FLL j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6747</th>\n",
       "      <td>Delta</td>\n",
       "      <td>@JetBlue I hope so because I fly very often an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     airline                                               text\n",
       "6746   Delta  @JetBlue Yesterday on my way from EWR to FLL j...\n",
       "6747   Delta  @JetBlue I hope so because I fly very often an..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8966</th>\n",
       "      <td>US Airways</td>\n",
       "      <td>@USAirways  is there a better time to call? My...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8967</th>\n",
       "      <td>US Airways</td>\n",
       "      <td>@USAirways and when will one of these agents b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         airline                                               text\n",
       "8966  US Airways  @USAirways  is there a better time to call? My...\n",
       "8967  US Airways  @USAirways and when will one of these agents b..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11879</th>\n",
       "      <td>American</td>\n",
       "      <td>@AmericanAir why would I even consider continu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11880</th>\n",
       "      <td>American</td>\n",
       "      <td>@AmericanAir we've already made other arrangem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        airline                                               text\n",
       "11879  American  @AmericanAir why would I even consider continu...\n",
       "11880  American  @AmericanAir we've already made other arrangem..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "airlines=maindf.airline.unique()\n",
    "for al in airlines:\n",
    "    display(maindf.loc[maindf['airline']==al,['airline','text']].head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All 'Delta' entries are wrong. They are actually tweets to @JetBlue. Change Delta to Jetblue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6746</th>\n",
       "      <td>JetBlue</td>\n",
       "      <td>@JetBlue Yesterday on my way from EWR to FLL j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6747</th>\n",
       "      <td>JetBlue</td>\n",
       "      <td>@JetBlue I hope so because I fly very often an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6748</th>\n",
       "      <td>JetBlue</td>\n",
       "      <td>@JetBlue flight 1041 to Savannah, GA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      airline                                               text\n",
       "6746  JetBlue  @JetBlue Yesterday on my way from EWR to FLL j...\n",
       "6747  JetBlue  @JetBlue I hope so because I fly very often an...\n",
       "6748  JetBlue               @JetBlue flight 1041 to Savannah, GA"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "maindf.airline.replace(to_replace='Delta', value='JetBlue', inplace=True)\n",
    "display(maindf.loc[maindf['airline']=='JetBlue',['airline','text']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some quick trend analysis and topic modelling\n",
    "\n",
    "Extract all text in the 'text' column and make a lit; display a few"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@VirginAmerica What @dhepburn said.',\n",
       " \"@VirginAmerica plus you've added commercials to the experience... tacky.\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_documents=list(maindf.text)\n",
    "\n",
    "documents=orig_documents.copy()\n",
    "documents[:2]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First some text preprocessing\n",
    "\n",
    "- Remove special characters\n",
    "- Lower case text\n",
    "- Remove numeric data\n",
    "- tokenize the phrases\n",
    "- remove less than 3 characters long\n",
    "- Lemmatize words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### display the cleaned tokenized ... few rows from top and bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup and tokenize the documents\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "STOPWORDS=stopwords.words('english')\n",
    "def clean_docs(documents):\n",
    "    \n",
    "    # Split the documents into tokens.\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for idx in range(len(documents)):\n",
    "        documents[idx] = documents[idx].lower()             # Convert to lowercase.\n",
    "        documents[idx]=re.sub(r'http\\S+', '',documents[idx]) # remove URL\n",
    "        documents[idx] = tokenizer.tokenize(documents[idx])  # Split into words.\n",
    "\n",
    "    # Remove numbers, but not words that contain numbers.\n",
    "    documents = [[token for token in doc if not token.isnumeric()] for doc in documents]\n",
    "\n",
    "    # Remove and words upto 2 character long\n",
    "    documents = [[token for token in doc if len(token) > 2] for doc in documents]\n",
    "\n",
    "    # Remove stopwords\n",
    "    documents=[[token for token in doc if token not in STOPWORDS] for doc in documents]\n",
    "\n",
    "    # Lemmatize the documents (stemming is crude and produces unreadable words)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    documents = [[lemmatizer.lemmatize(token) for token in doc] for doc in documents]\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['virginamerica', 'dhepburn', 'said'], ['virginamerica', 'plus', 'added', 'commercial', 'experience', 'tacky']]\n",
      "[['americanair', 'money', 'change', 'flight', 'answer', 'phone', 'suggestion', 'make', 'commitment'], ['americanair', 'ppl', 'need', 'know', 'many', 'seat', 'next', 'flight', 'plz', 'put', 'standby', 'people', 'next', 'flight']]\n"
     ]
    }
   ],
   "source": [
    "documents=clean_docs(documents)\n",
    "print(documents[:2])\n",
    "print(documents[-2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove airline names from text, e.g. 'americanair', 'virginamerica'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags to remove:\n",
      "['virginamerica', 'united', 'southwestair', 'jetblue', 'usairways', 'americanair']\n",
      "tags removed:\n",
      "[['dhepburn', 'said'], ['plus', 'added', 'commercial', 'experience', 'tacky']]\n",
      "[['money', 'change', 'flight', 'answer', 'phone', 'suggestion', 'make', 'commitment'], ['ppl', 'need', 'know', 'many', 'seat', 'next', 'flight', 'plz', 'put', 'standby', 'people', 'next', 'flight']]\n"
     ]
    }
   ],
   "source": [
    "airlines=maindf.airline.unique()\n",
    "altags=[]\n",
    "for al in airlines:\n",
    "    x=maindf.loc[maindf['airline']==al,['text']].text.tolist()[0].partition(' ')[0]\n",
    "    x=x.lower()\n",
    "    x=x[1:]         #remove '@'\n",
    "    altags.append(x)\n",
    "print('tags to remove:')\n",
    "print(altags)\n",
    "\n",
    "STOPWORDS.extend(altags)\n",
    "STOPWORDS.extend(['http'])\n",
    "documents=[[token for token in doc if token not in STOPWORDS] for doc in documents]\n",
    "print('tags removed:')\n",
    "print(documents[:2])\n",
    "print(documents[-2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a document-term matrix\n",
    "\n",
    "- Calculate bigrams in the documents and add them in the document.\n",
    "- construct a document-term matrix using Gensim's Dictionary() function\n",
    "  - will assign a unique integer id to each unique token and assign word counts\n",
    "- converted Dictionary into a bag-of-words\n",
    "  -  will create a list of vectors equal to the number of documents. Eac document vector is a series of tuples (term ID, term frequency)\n",
    "- Remove rare & common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import Phrases\n",
    "\n",
    "def make_docmatrix(documents,ngram=2):\n",
    "    \n",
    "    if ngram>2:\n",
    "        print('Error: ngram should be 1 or 2')\n",
    "        return\n",
    "    if ngram==2:\n",
    "        # Adding bigrams documents, the ones that appear 20 times or more.\n",
    "        bigram = Phrases(documents, min_count=20)\n",
    "        for idx in range(len(documents)):\n",
    "            for token in bigram[documents[idx]]:\n",
    "                if '_' in token:\n",
    "                    # Token is a bigram, add to document.\n",
    "                    documents[idx].append(token)\n",
    "    \n",
    "    # For dictionary representation of the documents.\n",
    "    dictionary = Dictionary(documents)\n",
    "\n",
    "    # Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "    dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "\n",
    "    # Vectorize representation of words & n-grams.\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "    print('Number of unique tokens: %d' % len(dictionary))\n",
    "    print('Number of documents: %d' % len(corpus))\n",
    "    print(dictionary)\n",
    "    return documents,dictionary,corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 991\n",
      "Number of documents: 14640\n",
      "Dictionary(991 unique tokens: ['said', 'added', 'experience', 'plus', 'another']...)\n",
      "Done. corpus & dictionary constructed.\n"
     ]
    }
   ],
   "source": [
    "documents,dictionary,corpus=make_docmatrix(documents,ngram=2)\n",
    "print('Done. corpus & dictionary constructed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an LDA model\n",
    "\n",
    "The text data is preprocessed and ready for LDA model training. LSA is statistical, LDA is probablistic and usually shows better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing training logs to file: ldatrain.log\n",
      "num_topics: 10\n",
      "chunksize:  500\n",
      "passes:     100\n",
      "iterations: 400\n",
      "eval_every: 5\n",
      "Training started......\n",
      "Training done.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaMulticore\n",
    "\n",
    "import logging\n",
    "\n",
    "logfn='ldatrain.log'\n",
    "print('Writing training logs to file:',logfn)\n",
    "logging.basicConfig(filename=logfn,level=logging.DEBUG)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "#logger.setLevel(logging.DEBUG)\n",
    "#logging.debug(\"test\")\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 10\n",
    "chunksize = 500\n",
    "passes = 100\n",
    "iterations =400\n",
    "eval_every =5 \n",
    "\n",
    "print('num_topics:', num_topics)\n",
    "print('chunksize: ', chunksize)\n",
    "print('passes:    ', passes)\n",
    "print('iterations:', iterations)\n",
    "print('eval_every:', eval_every)\n",
    "\n",
    "print('Training started......')\n",
    "model = LdaMulticore(corpus=corpus,\n",
    "                 id2word=dictionary,\n",
    "                 chunksize=chunksize,\n",
    "                 #alpha='auto', \n",
    "                 eta='auto',\n",
    "                 iterations=iterations, \n",
    "                 num_topics=num_topics,\n",
    "                 passes=passes, \n",
    "                 eval_every=eval_every,\n",
    "                 workers=11,\n",
    "                 random_state=123)\n",
    "\n",
    "logger.setLevel(logging.CRITICAL) # set logging back to critical to reduce messages\n",
    "print('Training done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets see the top topics(trends) identified by the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment trend  1: hold-hour-phone-help-get-trying-time-wait-call-minute\n",
      "Comment trend  2: flight-cancelled-cancelled_flightled-flightled-tomorrow-get-rebooked-got-dfw-today\n",
      "Comment trend  3: flight-delayed-gate-time-plane-delay-hour-crew-waiting-connection\n",
      "Comment trend  4: flight-seat-late-get-home-change-late_flight-flighted-cancelled_flighted-cancelled\n",
      "Comment trend  5: call-problem-back-number-email-yes-get-sent-booking-say\n",
      "Comment trend  6: bag-plane-luggage-hour-lost-baggage-still-jfk-weather-amp\n",
      "Comment trend  7: great-thanks-time-right-good-look-trip-airline-hope-flying\n",
      "Comment trend  8: thanks-thank-help-agent-please-guy-much-need-love-class\n",
      "Comment trend  9: would-like-fly-staff-better-want-never-anything-people-make\n",
      "Comment trend 10: service-customer-customer_service-ever-worst-airline-experience-attendant-fleek-fleet\n"
     ]
    }
   ],
   "source": [
    "# Extract the top 10 topics/trends based on topic coherence\n",
    "topcohr=model.top_topics(corpus=corpus, dictionary=dictionary, topn=10)\n",
    "\n",
    "# make a dictionary of trends with most significant words\n",
    "topcohrDict={}\n",
    "for idx, elem in enumerate(topcohr):\n",
    "    topic_words=[]\n",
    "    for y in elem[0]:\n",
    "        topic_words.append(y[1])\n",
    "    topcohrDict[idx]=topic_words\n",
    "\n",
    "# display the trends    \n",
    "for idx,elem in enumerate(topcohrDict.values()):\n",
    "    print('Comment trend {:2d}: {}'.format(idx+1,'-'.join(elem)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create topic query functions for new topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### for creating a df with topic numbers, words, probablities\n",
    "topics_matrix=model.show_topics(formatted=False, num_words=20)\n",
    "topdf=pd.DataFrame()\n",
    "topidx=[]\n",
    "topwords=[]\n",
    "wordprob=[]\n",
    "for idx,topic in enumerate(topics_matrix):\n",
    "    #print('idx=',idx)\n",
    "    for tupl in topic[1]:\n",
    "        #print(tupl[1])\n",
    "        topidx.append(idx)\n",
    "        topwords.append(tupl[0])\n",
    "        wordprob.append(tupl[1])\n",
    "topdf['topidx']=topidx\n",
    "topdf['topwords']=topwords\n",
    "topdf['wordprob']=wordprob\n",
    "\n",
    "def topic_print(topic):\n",
    "    topids=[]\n",
    "    tprobs=[]\n",
    "    qrydf=pd.DataFrame()\n",
    "    for tup in topic:\n",
    "        topids.append(tup[0])\n",
    "        tprobs.append(tup[1])\n",
    "    qrydf['topids']=topids\n",
    "    qrydf['tprobs']=tprobs\n",
    "    qrydf.sort_values(by='tprobs',ascending=False,inplace=True)\n",
    "    t1=qrydf.topids.iloc[0]\n",
    "    t2=qrydf.topids.iloc[1]\n",
    "    \n",
    "    print('+++++++++++++++++Trends/topics found++++++++++++')\n",
    "    for idx,val in enumerate([t1,t2]):\n",
    "        elem=list(topdf['topwords'].loc[topdf.topidx==val])\n",
    "        print('++Comment trend {:1d}: {}'.format(idx+1,'-'.join(elem)))\n",
    "    #print('\\n')\n",
    "\n",
    "def find_topics(query):\n",
    "    print('+++++++++++++++Original comment text++++++++++:')\n",
    "    print(query,'\\n')\n",
    "\n",
    "    query1=clean_docs(query)\n",
    "    tempcorpus = [dictionary.doc2bow(doc) for doc in query1]\n",
    "    query_topics=model[tempcorpus]\n",
    "    for topic in query_topics:\n",
    "        topic_print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find trends/topics in new complaints (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++++++++++++Original comment text++++++++++:\n",
      "['sitting in the plane at the gate for an hour'] \n",
      "\n",
      "+++++++++++++++++Trends/topics found++++++++++++\n",
      "++Comment trend 1: flight-delayed-gate-time-plane-delay-hour-crew-waiting-connection-boarding-min-miss-still-update-passenger-going-stuck-pilot-minute\n",
      "++Comment trend 2: bag-plane-luggage-hour-lost-baggage-still-jfk-weather-amp-get-left-one-checked-check-sitting-night-waiting-claim-lax\n"
     ]
    }
   ],
   "source": [
    "query=['sitting in the plane at the gate for an hour']\n",
    "find_topics(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++++++++++++Original comment text++++++++++:\n",
      "['cant find my luggage'] \n",
      "\n",
      "+++++++++++++++++Trends/topics found++++++++++++\n",
      "++Comment trend 1: bag-plane-luggage-hour-lost-baggage-still-jfk-weather-amp-get-left-one-checked-check-sitting-night-waiting-claim-lax\n",
      "++Comment trend 2: hold-hour-phone-help-get-trying-time-wait-call-minute-online-reservation-book-agent-flight-tried-website-change-line-amp\n"
     ]
    }
   ],
   "source": [
    "query=['cant find my luggage']\n",
    "find_topics(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++++++++++++Original comment text++++++++++:\n",
      "['took an hour to book a flight'] \n",
      "\n",
      "+++++++++++++++++Trends/topics found++++++++++++\n",
      "++Comment trend 1: hold-hour-phone-help-get-trying-time-wait-call-minute-online-reservation-book-agent-flight-tried-website-change-line-amp\n",
      "++Comment trend 2: bag-plane-luggage-hour-lost-baggage-still-jfk-weather-amp-get-left-one-checked-check-sitting-night-waiting-claim-lax\n"
     ]
    }
   ],
   "source": [
    "query=['took an hour to book a flight']\n",
    "find_topics(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets try an orginal text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++++++++++++Original comment text++++++++++:\n",
      "[\"@VirginAmerica hi! i'm so excited about your $99 LGA-&gt;DAL deal- but i've been trying 2 book since last week &amp; the page never loads. thx!\"] \n",
      "\n",
      "+++++++++++++++++Trends/topics found++++++++++++\n",
      "++Comment trend 1: hold-hour-phone-help-get-trying-time-wait-call-minute-online-reservation-book-agent-flight-tried-website-change-line-amp\n",
      "++Comment trend 2: bag-plane-luggage-hour-lost-baggage-still-jfk-weather-amp-get-left-one-checked-check-sitting-night-waiting-claim-lax\n"
     ]
    }
   ],
   "source": [
    "query=[orig_documents[55]]\n",
    "find_topics(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick play with Word2Vec for topic modelling\n",
    "\n",
    "Word2Vec is a two layer neural net with pre-trained several hundered dimensional word embeddings. The embedding are context rich.\n",
    "\n",
    "Will quickly train a Word2vec on the corpora of complaint documents, with 100 dimensional vector per word and use skip gram (instead of CBOW). Skip gram provides context words from a single word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained...print a few words:\n",
      "vocabulary size= 2678\n",
      "['flight', 'get', 'hour', 'thanks', 'cancelled', 'service', 'time', 'customer', 'help', 'bag', 'plane', 'amp', 'hold', 'need', 'thank', 'one', 'still', 'call', 'please', 'customer_service']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "w2vmodel = gensim.models.word2vec.Word2Vec(sentences=documents, \n",
    "                                           size=100,  # 100 dimensional vector for each token\n",
    "                                           sg=1,      # use skip-gram instead of CBOW\n",
    "                                           window=5,  # Maximum distance between the current and predicted word within a sentence\n",
    "                                           workers=12 # CPU threads\n",
    "                                          )\n",
    "print('Model trained...print a few words:')\n",
    "wordlist = list(w2vmodel.wv.index2word)\n",
    "print('vocabulary size=',len(wordlist))\n",
    "print(wordlist[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try word2vec model with the same queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate topics using word2vec trained model\n",
    "def w2v_pred(query):\n",
    "    print('++Original Text:',query)\n",
    "    print('')\n",
    "    clean_query=clean_docs(query)[0]\n",
    "\n",
    "    print('++Word2Vec produced context words:')\n",
    "    display(w2vmodel.wv.most_similar(positive=clean_query,topn=10))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++Original Text: ['sitting in the plane at the gate for an hour']\n",
      "\n",
      "++Word2Vec produced context words:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('tarmac', 0.9787759184837341),\n",
       " ('runway', 0.957625687122345),\n",
       " ('sitting_plane', 0.9564936757087708),\n",
       " ('half', 0.9446218013763428),\n",
       " ('waiting', 0.9443740844726562),\n",
       " ('left', 0.9437344074249268),\n",
       " ('landed', 0.9436891078948975),\n",
       " ('sat', 0.9414244890213013),\n",
       " ('boarded', 0.9312993288040161),\n",
       " ('ground', 0.9284708499908447)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query=['sitting in the plane at the gate for an hour']\n",
    "w2v_pred(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++Original Text: ['cant find my luggage']\n",
      "\n",
      "++Word2Vec produced context words:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('promised', 0.9742435812950134),\n",
       " ('went', 0.9700623750686646),\n",
       " ('delivered', 0.9678585529327393),\n",
       " ('drop', 0.9677227139472961),\n",
       " ('international', 0.9675685167312622),\n",
       " ('dropped', 0.9672185182571411),\n",
       " ('midnight', 0.9671964645385742),\n",
       " ('land', 0.9660167098045349),\n",
       " ('advance', 0.9650110602378845),\n",
       " ('one', 0.9643750786781311)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query=['cant find my luggage']\n",
    "w2v_pred(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++Original Text: [\"@VirginAmerica hi! i'm so excited about your $99 LGA-&gt;DAL deal- but i've been trying 2 book since last week &amp; the page never loads. thx!\"]\n",
      "\n",
      "++Word2Vec produced context words:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('3rd', 0.9923524856567383),\n",
       " ('wed', 0.9887734055519104),\n",
       " ('track', 0.9883015155792236),\n",
       " ('pushing', 0.9882409572601318),\n",
       " ('birmingham', 0.9878718852996826),\n",
       " ('button', 0.987572968006134),\n",
       " ('client', 0.9874402284622192),\n",
       " ('jacksonville', 0.9873881340026855),\n",
       " ('operating', 0.9873400330543518),\n",
       " ('wth', 0.986469030380249)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query=[orig_documents[55]]\n",
    "w2v_pred(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec conclusion: Not too shabby for such quick and dirty try. By spending a little more time, hypertuning and trying different parameters it has good potential to find topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "Interesting results considering an hours work. \n",
    "\n",
    "Many things could be done to improve model:\n",
    "1. We can get (or scrape) the airline's response to these queries/comments a predictive model can be generated which can respond (a chatbot)\n",
    "2. Hyperparameter tuning\n",
    "3. Try more iterations, and different batch sizes\n",
    "4. Trying trigram and 4-grams\n",
    "5. In this quick example I just used 'text' data, however, all the non-text data can be used to better the model using time, day of week, orignation, destination and other fields\n",
    "6. Improve, tune, train the word2vec model\n",
    "\n",
    "#### Business uses:\n",
    "- A weekly report to airline's executive mgmt indicating the trends/issues in complaint data (from twitter channel)\n",
    "- A dialog system (chatbot)\n",
    "- Redirect, classify and summarize complaint/inquiry text from airline's support pages "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
